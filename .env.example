# Environment Configuration for Local VLM Deployment
# Copy this file to .env (or the values will be used as defaults)

# VLM Provider - Use "OpenAI" for local OpenAI-compatible servers
VLM_PROVIDER=OpenAI

# VLM Base URL - Point to your local Model-Server
VLM_BASE_URL=http://localhost:8001/v1

# VLM API Key - Set to any value for local servers (not validated)
VLM_API_KEY=none

# VLM Model Name - Must match the model served by your server
VLM_MODEL_NAME=ui-tars
